I"ê<p class="text-left"><i>Authors: Anirudha Majumdar, Sumeet Singh, Ajay Mandlekar, Marco Pavone</i></p>

<p>The literature on Inverse Reinforcement Learning (IRL) typically assumes that humans take actions in order to minimize the expected value of a cost function, i.e., that humans are risk neutral. Yet, in practice, humans are often far from being risk neutral. To fill this gap, the objective of this paper is to devise a framework for risk-sensitive IRL in order to explicitly account for an expertâ€™s risk sensitivity. To this end, we propose a flexible class of models based on coherent risk metrics, which allow us to capture an entire spectrum of risk preferences from risk-neutral to worst-case. We propose efficient algorithms based on Linear Programming for inferring an expertâ€™s underlying risk metric and cost function for a rich class of static and dynamic decision-making settings. The resulting approach is demonstrated on a simulated driving game with ten human participants. Our method is able to infer and mimic a wide range of qualitatively different driving styles from highly risk-averse to risk-neutral in a data-efficient manner. Moreover, comparisons of the Risk-Sensitive (RS) IRL approach with a risk-neutral model show that the RS-IRL framework more accurately captures observed participant behavior both qualitatively and quantitatively.</p>

<p>[<b><a href="/static/papers/62.pdf">Full Paper</a></b> | <b><a href="/static/slides/62.mp4">Slides</a></b>]</p>

<div id="disqus_thread"></div>
<script>
var disqus_config = function () {
this.page.url = "http://roboticsconference.org/program/papers/62/";
this.page.identifier = "/program/papers/62";
};
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://rss-2017.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

:ET