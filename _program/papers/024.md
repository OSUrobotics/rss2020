---
layout: paper
title: "Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations"
invisible: true
---
<head>
<style>
* {
  box-sizing: border-box;
}

#myInput {
  background-position: 10px 10px;
  background-repeat: no-repeat;
  width: 100%;
  font-size: 100%;
  padding: 12px 20px 12px 40px;
  border: 1px solid #ddd;
  margin-bottom: 12px;
}

#myTable0 #myTable1 {
  border-collapse: collapse;
  width: 100%;
  border: 1px solid #ddd;
  font-size: 100%;
}

#myTable0 th, #myTable0 td, #myTable1 th, #myTable1 td {
  text-align: left;
  padding: 12px;
}

#myTable0 tr, #myTable1 tr {
  border-bottom: 1px solid #ddd;
}

#myTable0 tr.header, #myTable0 tr:hover, #myTable1 tr.header, #myTable1 tr:hover {
  background-color: #f1f1f1;
}

#eventcounter1 a {
    font-size: 12px;
    color: #ffffff;
    display: block;
}

#eventcounter1 a:hover {
    text-decoration: none;
}

#eventcounter2 a {
    font-size: 12px;
    color: #ffffff;
    display: block;
}

#eventcounter2 a:hover {
    text-decoration: none;
}

</style>
</head>

<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Zhenyu Jiang <span style="color:gray; font-size: 85%">(University of Texas at Austin)</span><span style="color:gray; font-size: 100%">,</span><br>  Yifeng Zhu <span style="color:gray; font-size: 85%">(University of Texas at Austin)</span><span style="color:gray; font-size: 100%">,</span><br>  Maxwell Svetlik <span style="color:gray; font-size: 85%">(University of Texas at Austin)</span><span style="color:gray; font-size: 100%">,</span><br>  Kuan Fang <span style="color:gray; font-size: 85%">(Stanford University)</span><span style="color:gray; font-size: 100%">,</span><br>  Yuke Zhu <span style="color:gray; font-size: 85%">(University of Texas at Austin)</span>
</i></span>
</td>
<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss17/p024.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a><br> <a href="https://sites.google.com/view/rpl-giga2021"><img src="{{ site.baseurl }}/images/website_link.png" alt="Paper Website" width = "33"  height = "40"/></a><br>    </td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#024</td>
</tr>
</table>

<table width="80%" style="margin-top: 20px; margin-left: auto; margin-right: auto;">
                                          <tr><td style="text-align:center;"><a href="{{ site.baseurl }}/program/posters3/">Interactive Poster Session III</a></td> 
                                              <td style="text-align:center;"><a href="{{ site.baseurl }}/program/posters6/">Interactive Poster Session VI</a></td></tr>
<tr><td><p style="text-align: center; font-size: 10px; margin-top: 0px;" id="eventcounter1"><a>0d 00h 00m</a></p></td><td><p style="text-align: center; font-size: 10px; margin-top: 0px;" id="eventcounter2"><a>0d 00h 00m</a></p></td></tr></table>
<br>


### Abstract
Grasp detection in clutter requires the robot to reason about the 3D scene from incomplete and noisy perception. In this work, we draw insight that 3D reconstruction and grasp learning are two intimately connected tasks, both of which require a fine-grained understanding of local geometry details. We thus propose to utilize the synergies between grasp affordance and 3D reconstruction through multi-task learning of a shared representation. Our model takes advantage of deep implicit functions, a continuous and memory-efficient representation, to enable differentiable training of both tasks. We train the model on self-supervised grasp trials data in simulation. Evaluation is conducted on a clutter removal task, where the robot clears cluttered objects by grasping them one at a time. The experimental results in simulation and on the real robot have demonstrated that the use of implicit neural representations and joint learning of grasp affordance and 3D reconstruction have led to state-of-the-art grasping results. Our method outperforms baselines by over 10% in terms of grasp success rate.
{: style="color:gray; font-size: 120%; text-align: justified;"}




### Spotlight Presentation
<center><span style="font-size:smaller;"><i>This video will be released on July 12th.</i></span></center><br>
<iframe width="100%" height="400" src="https://www.youtube.com/embed/PdU_ZC6Jvck" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Links
* [Project page](https://sites.google.com/view/rpl-giga2021)


<table width="100%" style="margin-top:40px;">
 <tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/023/">
<img src="{{ site.baseurl }}/images/previous_paper_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/025/">
    <img src="{{ site.baseurl }}/images/next_paper_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
</tr>
</table>


<script>
var startDate1 = new Date("2021-07-13 08:15:00 UTC-0700").getTime();
var finDate1 = new Date("2021-07-13 09:30:00 UTC-0700").getTime();

// Update the count down every 1 second
var x1 = function() {

  // Get today's date and time
  var now1 = new Date().getTime();
    
  var distToStart1 = startDate1 - now1;
  if (distToStart1 > 0) {

      var days = Math.floor(distToStart1 / (1000 * 60 * 60 * 24));
      var hours = Math.floor((distToStart1 % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
      var minutes = Math.floor((distToStart1 % (1000 * 60 * 60)) / (1000 * 60));
   
      document.getElementById("eventcounter1").innerHTML = "<a><span style='color: #aaaaaa;'>" + days + "d " + hours + "h " + minutes + "m</span></a>" ;
      setTimeout(x1, 5000); 
    
  } else {

        var distToEnd1 = finDate1 - now1;

        if (distToEnd1 > 0) {
            document.getElementById("eventcounter1").innerHTML = '<img src="{{ site.baseurl }}/images/live-icon-small.gif" alt="Event is Live" width="64" height=17"><a><span style="color: #ffaaaa;">'+ distToEnd1 +'</span></a> ';
            setTimeout(x1, 30000); 
        }
        else
        { 
            document.getElementById("eventcounter1").innerHTML = "<a><span style='color: #aaaaaa;'>Now concluded</span></a>";
        }
  }
};

setTimeout(x1,0);
</script>

    
<script>
var startDate2 = new Date("2021-07-15 07:00:00 UTC-0700").getTime();
var finDate2 = new Date("2021-07-15 08:15:00 UTC-0700").getTime();

// Update the count down every 1 second
var x2 = function() {

  // Get today's date and time
  var now2 = new Date().getTime();
    
  var distToStart2 = startDate2 - now2;
  if (distToStart2 > 0) {

      var days = Math.floor(distToStart2 / (1000 * 60 * 60 * 24));
      var hours = Math.floor((distToStart2 % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
      var minutes = Math.floor((distToStart2 % (1000 * 60 * 60)) / (1000 * 60));
   
      document.getElementById("eventcounter2").innerHTML = "<a><span style='color: #aaaaaa;'>" + days + "d " + hours + "h " + minutes + "m</span></a>" ;
      setTimeout(x2, 5000); 
    
  } else {

        var distToEnd2 = finDate2 - now2;

        if (distToEnd2 > 0) {
            document.getElementById("eventcounter2").innerHTML = '<img src="{{ site.baseurl }}/images/live-icon-small.gif" alt="Event is Live" width="64" height=17"><a><span style="color: #ffaaaa;">'+ distToEnd2 +'</span></a> ';
            setTimeout(x2, 30000); 
        }
        else
        { 
            document.getElementById("eventcounter2").innerHTML = "<a><span style='color: #aaaaaa;'>Now concluded</span></a>";
        }
  }
};

setTimeout(x2,0);
</script>

    