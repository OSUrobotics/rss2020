---
layout: paper
title: "Safe Policies for Factored Partially Observable Stochastic Games"
invisible: true
---
<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Steven Carr <span style="color:gray; font-size: 85%">(University of Texas at Austin)</span><span style="color:gray; font-size: 100%">,</span><br>  Nils Jansen <span style="color:gray; font-size: 85%">(Radboud University Nijmegen)</span><span style="color:gray; font-size: 100%">,</span><br>  Sudarshanan Bharadwaj <span style="color:gray; font-size: 85%">(University of Texas at Austin)</span><span style="color:gray; font-size: 100%">,</span><br>  Matthijs  T. J. Spaan <span style="color:gray; font-size: 85%">(Delft University of Technology)</span><span style="color:gray; font-size: 100%">,</span><br>  Ufuk Topcu <span style="color:gray; font-size: 85%">(University of Texas at Austin)</span>
</i></span>
</td>
<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss17/p079.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br>     </td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#079</td>
</tr>
</table>


### Abstract
We study planning problems where a controllable agent operates under partial observability and interacts with an uncontrollable opponent, also referred to as the adversary. The agent has two distinct objectives: To maximize an expected value and to adhere to a safety specification. Multi-objective partially observable stochastic games (POSGs) formally model such problems. Yet, even for a single objective, the task of computing suitable policies for POSGs is theoretically hard and computationally intractable in practice. Using a factored state-space representation, we define a decoupling scheme for the POSG state space that&mdash;under certain assumptions on the observability and the reward structure&mdash;separates the state components relevant for the reward from those relevant for safety. This decoupling affects the possibility to compute provably safe and reward-optimal policies in a tractable two-stage approach. In particular, on the fully observable components related to safety, we exactly compute the set of policies that captures all possible safe choices against the opponent. We restrict the agent's behavior to these safe policies and project the POSG to a partially observable Markov decision process (POMDP). Any reward-maximal policy for the POMDP is then guaranteed to be safe and reward-maximal for the POSG.  We showcase our approach's feasibility using high-fidelity simulations of two case studies that concern UAV path planning and autonomous driving. Moreover, to demonstrate the practical applicability, we design a physical experiment involving a robot decision making problem under energy constraints that is motivated by a paired helicopter with NASA's Perseverance Mars rover.
{: style="color:gray; font-size: 120%; text-align: justified;"}



<table width="100%">
 <tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/078/">
<img src="{{ site.baseurl }}/images/previous_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/080/">
    <img src="{{ site.baseurl }}/images/next_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
</tr>
</table>