---
layout: paper
title: "Robust Value Iteration for Continuous Control Tasks"
invisible: true
---
<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Michael Lutter <span style="color:gray; font-size: 85%">(TU Darmstadt)</span><span style="color:gray; font-size: 100%">,</span><br>  Shie Mannor <span style="color:gray; font-size: 85%">(Technion)</span><span style="color:gray; font-size: 100%">,</span><br>  Jan Peters <span style="color:gray; font-size: 85%">(TU Darmstadt)</span><span style="color:gray; font-size: 100%">,</span><br>  Dieter Fox <span style="color:gray; font-size: 85%">(NVIDIA)</span><span style="color:gray; font-size: 100%">,</span><br>  Animesh Garg <span style="color:gray; font-size: 85%">(University of Toronto, Vector Institute, NVIDIA)</span>
</i></span>
</td>
<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss17/p007.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br> <a href="https://sites.google.com/view/rfvi"><img src="{{ site.baseurl }}/images/website_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br>    </td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#007</td>
</tr>
</table>


### Abstract
When transferring a control policy from simulation to a physical system, this policy needs to be robust to variations in the dynamics to perform well. Commonly, the optimal policy overfits to the approximate model and the corresponding state-distribution. Therefore, the policy fails when transferred to the physical system.  In this paper, we are presenting robust value iteration. This approach uses dynamic programming to compute the optimal value function on the compact state domain and incorporates adversarial perturbations of the system dynamics. The adversarial perturbations cause the resulting optimal policy to be robust to changes in the dynamics. Utilizing the continuous time perspective of reinforcement learning, we derive the optimal perturbations for the states, actions, observations and model parameters in closed-form. The resulting algorithm does not require discretization of states or actions. Therefore, the optimal adversarial perturbations can be efficiently incorporated in the min-max value function update.  We apply the resulting algorithm to the physical Furuta Pendulum and cartpole. By changing the masses of the systems we evaluate the quantitative and qualitative performance across different model parameters. We show that robust value iteration is more robust compared to deep reinforcement learning algorithm and the non-robust version of the algorithm. 
{: style="color:gray; font-size: 120%; text-align: justified;"}



<table width="100%">
 <tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/006/">
<img src="{{ site.baseurl }}/images/previous_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/008/">
    <img src="{{ site.baseurl }}/images/next_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
</tr>
</table>