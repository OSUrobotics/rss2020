---
layout: paper
title: "Interpreting and Predicting Tactile Signals via a Physics-Based and Data-Driven Framework"
invisible: true
---
*Yashraj Narang (NVIDIA); Karl Van Wyk (NVIDIA); Arsalan Mousavian (NVIDIA); Dieter Fox (NVIDIA)*
{: style="color:black; font-size: 120%; text-align: center;"}

### Abstract
<html><p style="color:gray; font-size: 120%; text-align: justified;">
High-density afferents in the human hand have long been regarded as essential for human grasping and manipulation abilities. In contrast, robotic tactile sensors are typically used to provide low-density contact data, such as center-of-pressure and resultant force. Although useful, this data does not exploit the rich information content that some tactile sensors (e.g., the SynTouch BioTac) naturally provide. This research extends robotic tactile sensing beyond reduced-order models through 1) the automated creation of a precise tactile dataset for the BioTac over diverse physical interactions, 2) a 3D finite element (FE) model of the BioTac, which complements the experimental dataset with high-resolution, distributed contact data, and 3) neural-network-based mappings from raw BioTac signals to low-dimensional experimental data, and more importantly, high-density FE deformation fields. These data streams can provide a far greater quantity of interpretable information for grasping and manipulation algorithms than previously accessible.
</p></html>

### Supplementary Video
<iframe width="560" height="315" src="https://www.youtube.com/embed/wLA-WKaeyN4 " frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Supplementary Links
**[Webpage Link](https://sites.google.com/nvidia.com/tactiledata)**  



