---
layout: paper
title: "Explaining Multi-stage Tasks by Learning Temporal Logic Formulas from Suboptimal Demonstrations"
invisible: true
---
*[Glen Chou](http://web.eecs.umich.edu/~gchou/), [Necmiye Ozay](http://web.eecs.umich.edu/~necmiye/), [Dmitry Berenson](http://web.eecs.umich.edu/~dmitryb/)*
{: style="color:black; font-size: 120%; text-align: center;"}

<table width="20%"> <tr>
<td style="width: 20%; text-align: center;"><a href="1303"><img src="{{ site.baseurl }}/images/paper_link.png"
width = "50"  height = "60"/> </a> </td>

<td style="width: 20%; text-align: center;"><a href="nan"><img src="{{ site.baseurl }}/images/pheedloop_link.png"
width = "70"  height = "60"/> </a> </td>

</tr></table>

### Abstract
<html><p style="color:gray; font-size: 120%; text-align: justified;">
We present a method for learning to perform multi-stage tasks from demonstrations by learning the logical structure and atomic propositions of a consistent linear temporal logic (LTL) formula. The learner is given successful but potentially suboptimal demonstrations, where the demonstrator is optimizing a cost function while satisfying the LTL formula, and the cost function is uncertain to the learner. Our algorithm uses the Karush-Kuhn-Tucker (KKT) optimality conditions of the demonstrations together with a counterexample-guided falsification strategy to learn the atomic proposition parameters and logical structure of the LTL formula, respectively. We provide theoretical guarantees on the conservativeness of the recovered atomic proposition sets, as well as completeness in the search for finding an LTL formula consistent with the demonstrations. We evaluate our method on high-dimensional nonlinear systems by learning LTL formulas explaining multi-stage tasks on 7-DOF arm and quadrotor systems and show that it outperforms competing methods for learning LTL formulas from positive examples.
</p></html>

### Supplementary Video
<iframe width="700" height="400" src="https://www.youtube.com/embed/cpUEcWCUMqc " frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Paper Reviews
<details><summary style="font-size:20px;"><b> Review 2</b></summary>
<p style="color:gray; font-size: 120%; text-align: justified;">
This paper studies the problem of learning multi-stage tasks given aset of positive demonstrations from an approximately optimaldemonstrator.Specifically the paper studies a variant of Apprenticeship Learning/Inverse Reinforcement Learning (IRL) where parts of the task are describedby a fragment of Linear Temporal Logic.This follows a growing body of literature studying similar problems toaddress the fact traditional IRL (which learns rewards) does noteasily extend to sequential tasks.The approach taken in this paper is at a high level straightforward,but technically impressive. The authors propose:1. Parameterizing the space of cost functions and specifications.2. Searching for parameters that "explain" the demonstrations.However, like tradtional IRL - or perhaps more so - the notion of"explains" is highly degenerate.To alleviate this, the authors propose searching for parameters thatmake the demonstrator "approximately-optimal" where optimality isindependently evaluated both at level of low level control and withrespect to the specification.The low level critera optimality critera is given as the KKTconditions, which is never fully justified. I must admit, that whilethorough, the exposition in this section was quite terse and at somepoints dizzying. Even being somewhat familar with the embedding oftemporal logic constraints as Mixed Interger Constraints, I found thispart hard to follow.The specification optimality critera later comes in the form of"minimality" in the atomic propositions "visited". While I'm notcertain, believe that this minimality might correspond to the pathlength in the monitoring buchi-automata, subject to dynamicfeasbility.The ultimate algorithm resembles counter example guided inductivesynthesis. The essential idea is to check if there exists a formulawith a bounded syntatic dag size that is approximately optimal - wherethe "approximate" comes in the form of a meta-parameter bounding thedistance from optimality. The refuted formula are used to synthesizecounter example trajectories which serve to tighten the concept class.If no formula of a particular DAG size exists, the DAG size isincreased.This length based search, which uses a previously establish SATencoding of possible parse trees, is both systematic and serves toregularize the learner, since larger formulas are more expressive -and thus easier to overfit.Finally, the paper evaluates this technique on a number of impressiveexamples and demonstrates that the addition of goal directed behaviordoes indeed output perform a similar techniques for learning frompositive examples.My primary concerns are:1. How does this algorithm deal uncertainty in the dynamics. The   approximate KKL satisfaction seems like a potential solution, but   doesn't explicitly model agents accounting for risk.2. Is there a way to measure how "confident" the algorithm is in the   returned result. 3. Is there a way to robustify this algorithm to mis-labeled or unlabeled   examples. In particular, it seems to me that incorrectly refuting   a trajectory in step 10 of Alg 1 could have disastrous consequences   for learning algorithm, despite ample evidence in the rest of the   demonstrations. 
</p> </details>

<table width="100%"><tr><td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/96"> <img src="{{ site.baseurl }}/images/previous_icon.png" width = "120"  height = "90"/> </a> </td>

<td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers"> <img src="{{ site.baseurl }}/images/overview_icon.png" width = "120"  height = "90"/> </a> </td> 

<td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/98"> <img src="{{ site.baseurl }}/images/next_icon.png" width = "100"  height = "80"/> </a> </td> 

</tr></table>

