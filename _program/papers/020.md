---
layout: paper
title: "INVIGORATE: Interactive Visual Grounding and Grasping in Clutter"
invisible: true
---
<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Hanbo Zhang <span style="color:gray; font-size: 85%">(Xi'an Jiaotong University)</span><span style="color:gray; font-size: 100%">,</span><br>  Yunfan Lu <span style="color:gray; font-size: 85%">(National University of Singapore)</span><span style="color:gray; font-size: 100%">,</span><br>  Cunjun Yu <span style="color:gray; font-size: 85%">(National University of Singapore)</span><span style="color:gray; font-size: 100%">,</span><br>  David Hsu <span style="color:gray; font-size: 85%">(National University of Singapore)</span><span style="color:gray; font-size: 100%">,</span><br>  Xuguang Lan <span style="color:gray; font-size: 85%">(Xi'an Jiaotong University)</span><span style="color:gray; font-size: 100%">,</span><br>  Nanning Zheng <span style="color:gray; font-size: 85%">(Xi'an Jiaotong University)</span>
</i></span>
</td>
<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss17/p020.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br>  <a href="https://youtu.be/zYakh80SGcU"><img src="{{ site.baseurl }}/images/video_link.png" alt="Code" width = "50"  height = "60"/></a><br>   </td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#020</td>
</tr>
</table>


### Abstract
This paper presents INVIGORATE; a robot system that interacts with humans through natural language and grasps a specified object in clutter. The objects may occlude; obstruct; or even stack on top of one another. INVIGORATE embodies several challenges: (i) infer the target object among other occluding objects; from input language expressions and RGB images; (ii) infer object blocking relationships (OBRs) from the images; and (iii) synthesize a multi-step plan to ask questions that disambiguate the target object and to grasp it successfully. We train separate neural networks for object detection; for visual grounding; for question generation; and for OBR detection and grasping. They allow for unrestricted object categories and language expressions; subject to the training datasets. However; errors in visual perception and ambiguity in human languages are inevitable and negatively impact the robotâ€™s performance. To overcome these uncertainties; we build a partially observable Markov decision process (POMDP) that integrates the learned neural network modules. Through approximate POMDP planning; the robot tracks the history of observations and asks disambiguation questions in order to achieve a near-optimal sequence of actions that identify and grasp the target object. INVIGORATE combines the benefits of model-based POMDP planning and data-driven deep learning. Preliminary experiments with INVIGORATE on a Fetch robot show significant benefits of this integrated approach to object grasping in clutter with natural language interactions. A demonstration video is available online: <a href="https://youtu.be/zYakh80SGcU">https://youtu.be/zYakh80SGcU</a>.
{: style="color:gray; font-size: 120%; text-align: justified;"}



<table width="100%">
 <tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/019/">
<img src="{{ site.baseurl }}/images/previous_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/021/">
    <img src="{{ site.baseurl }}/images/next_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
</tr>
</table>