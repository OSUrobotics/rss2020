---
layout: paper
title: "Composable Energy Policies for Reactive Motion Generation and Reinforcement Learning "
invisible: true
---
<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Julen Urain <span style="color:gray; font-size: 85%">(TU Darmstadt)</span><span style="color:gray; font-size: 100%">,</span><br>  Puze Liu <span style="color:gray; font-size: 85%">(IAS, TU Darmstadt)</span><span style="color:gray; font-size: 100%">,</span><br>  Anqi Li <span style="color:gray; font-size: 85%">(University of Washington)</span><span style="color:gray; font-size: 100%">,</span><br>  Carlo D'Eramo <span style="color:gray; font-size: 85%">(TU Darmstadt)</span><span style="color:gray; font-size: 100%">,</span><br>  Jan Peters <span style="color:gray; font-size: 85%">(TU Darmstadt)</span>
</i></span>
</td>
<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss17/p052.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br> <a href="https://sites.google.com/view/composable-energy-policies/home"><img src="{{ site.baseurl }}/images/website_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br>    </td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#052</td>
</tr>
</table>


### Abstract
Reactive motion generation problems are usually solved by computing actions as a sum of policies. However; these policies are independent of each other and thus; they can have conflicting behaviors when summing their contributions together. We introduce Composable Energy Policies (CEP); a novel framework for modular reactive motion generation. CEP computes the control action by optimization over the product of a set of stochastic policies. This product of policies will provide a high probability to those actions that satisfy all the components and low probability to the others. Optimizing over the product of the policies avoids the detrimental effect of conflicting behaviors between policies choosing an action that satisfies all the objectives.  Besides; we show that CEP naturally adapts to the Reinforcement Learning problem allowing us to integrate; in a hierarchical fashion; any distribution as prior; from multimodal distributions to non-smooth distributions and learn a new policy given them
{: style="color:gray; font-size: 120%; text-align: justified;"}



<table width="100%">
 <tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/051/">
<img src="{{ site.baseurl }}/images/previous_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/053/">
    <img src="{{ site.baseurl }}/images/next_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
</tr>
</table>