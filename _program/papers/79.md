---
layout: paper
title: "3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans"
invisible: true
---
*[Antoni Rosinol](https://www.mit.edu/~arosinol/), [Luca Carlone](https://lucacarlone.mit.edu/)*
{: style="color:black; font-size: 120%; text-align: center;"}

### Abstract
<html><p style="color:gray; font-size: 120%; text-align: justified;">
We present a unified representation for actionable spatial perception: 3D Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent entities in the scene (e.g., objects, walls, rooms), and edges represent relations (e.g., inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs) extend this notion to represent dynamic scenes with moving agents (e.g., humans, robots), and to include actionable information to support planning and decision-making (e.g., spatio-temporal relations, topology at different levels of abstraction). Our second contribution is to provide the first end-to-end fully automatic Spatial PerceptIon eNgine (SPIN) to build a DSG from visual-inertial data. We integrate state-of-the-art techniques for object and human detection and pose estimation, and we describe how to robustly infer object, robot, and human nodes in crowded scenes. To the best of our knowledge, this is the first paper that reconciles visual-inertial SLAM and dense human mesh tracking. Moreover, we provide algorithms to obtain hierarchical representations of indoor environments (e.g., places, structures, rooms) and their relations. Our third contribution is to demonstrate the proposed spatial perception engine in a photo-realistic Unity-based simulator, where we assess its robustness and expressiveness. Finally, we discuss the implications of our proposal on modern robotics applications. We believe 3D Dynamic Scene Graphs can have a profound impact on planning and decision-making, human-robot interaction, long-term autonomy, and scene prediction.
</p></html>

### Supplementary Video
<iframe width="560" height="315" src="https://www.youtube.com/embed/SWbofjhyPzI " frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Supplementary Links
**[Webpage Link](http://web.mit.edu/sparklab/datasets/uHumans/)**  

**[Software Link](https://github.com/MIT-SPARK/Kimera-VIO)**



### Paper Reviews
<details><summary style="font-size:20px;"><b> Review 1</b></summary>
<p style="color:gray; font-size: 120%; text-align: justified;">
In my opinion, the weakest part of the proposal is the graph of places in Layer 3. A place is simply a 3D position in free space with a bounding box, but what size of bounding box? Looking at the large number of places in figure 2 it seems that the system is assuming a very small flying robot, while a bigger land robot would need a totally different places graph for path-planning. So, the claim that this places graph allows for path planning should be moderated.  Also, looking in detail at figure 2, we can observe that the system has found many paths between rooms that traverse crystal walls! Probably, good semantic mapping of this kind of environments would require detecting and explicitly representing doors.
</p> </details>

