---
layout: paper
title: "Manipulator-Independent Representations for Visual Imitation"
invisible: true
---
<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Yuxiang Zhou <span style="color:gray; font-size: 85%">(DeepMind)</span><span style="color:gray; font-size: 100%">,</span><br>  Yusuf Aytar <span style="color:gray; font-size: 85%">(DeepMind)</span><span style="color:gray; font-size: 100%">,</span><br>  Konstantinos Bousmalis <span style="color:gray; font-size: 85%">(DeepMind)</span>
</i></span>
</td>
<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss17/p002.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br> <a href="https://sites.google.com/view/mir4vi"><img src="{{ site.baseurl }}/images/website_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br>    </td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#002</td>
</tr>
</table>


### Abstract
Imitation learning is an effective tool for robotic learning tasks where specifying a reinforcement learning (RL) reward is not feasible or where the exploration problem is particularly difficult. Imitation; typically behavior cloning or inverse RL; derive a policy from a collection of first-person action-state trajectories. This is contrary to how humans and other animals imitate: we observe a behavior; even from other species; understand its perceived effect on the state of the environment; and figure out what actions our body can perform to reach a similar outcome. In this work; we explore the possibility of third-person visual imitation of manipulation trajectories; only from vision and without access to actions; demonstrated by embodiments different to the ones of our imitating agent. Specifically; we investigate what would be an appropriate representation method with which an RL agent can visually track trajectories of complex manipulation behavior &mdash;non-planar with multiple-object interactions&mdash; demonstrated by experts with different embodiments. We present a way to train manipulator-independent representations (MIR) that primarily focus on the change in the environment and have all the characteristics that make them suitable for cross-embodiment visual imitation with RL: domain-invariant; temporally smooth; and actionable. We show that with our proposed method our agents are able to imitate; with complex robot control; trajectories from a variety of embodiments and with significant visual and dynamics differences; e.g. simulation-to-reality gap.
{: style="color:gray; font-size: 120%; text-align: justified;"}



<table width="100%">
 <tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/001/">
<img src="{{ site.baseurl }}/images/previous_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/003/">
    <img src="{{ site.baseurl }}/images/next_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
</tr>
</table>