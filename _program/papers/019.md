---
layout: paper
title: "Learned Visual Navigation for Under-Canopy Agricultural Robots"
invisible: true
---
<head>
<style>
* {
  box-sizing: border-box;
}

#myInput {
  background-position: 10px 10px;
  background-repeat: no-repeat;
  width: 100%;
  font-size: 100%;
  padding: 12px 20px 12px 40px;
  border: 1px solid #ddd;
  margin-bottom: 12px;
}

#myTable {
  border-collapse: collapse;
  width: 100%;
  border: 1px solid #ddd;
  font-size: 100%;
}

#myTable th, #myTable td {
  text-align: left;
  padding: 12px;
}

#myTable tr {
  border-bottom: 1px solid #ddd;
}

#myTable tr.header, #myTable tr:hover {
  background-color: #f1f1f1;
}

#eventcounter1 a {
    font-size: 12px;
    color: #ffffff;
    display: block;
}

#eventcounter1 a:hover {
    text-decoration: none;
}

#eventcounter2 a {
    font-size: 12px;
    color: #ffffff;
    display: block;
}

#eventcounter2 a:hover {
    text-decoration: none;
}

</style>
</head>

<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Arun Narenthiran V Sivakumar <span style="color:gray; font-size: 85%">(UIUC)</span><span style="color:gray; font-size: 100%">,</span><br>  Sahil Modi <span style="color:gray; font-size: 85%">(UIUC)</span><span style="color:gray; font-size: 100%">,</span><br>  Mateus Valverde Gasparino <span style="color:gray; font-size: 85%">(UIUC)</span><span style="color:gray; font-size: 100%">,</span><br>  Che G Ellis <span style="color:gray; font-size: 85%">(EarthSense)</span><span style="color:gray; font-size: 100%">,</span><br>  Andres Eduardo Baquero Velasquez <span style="color:gray; font-size: 85%">(UIUC)</span><span style="color:gray; font-size: 100%">,</span><br>  Girish Chowdhary <span style="color:gray; font-size: 85%">(UIUC)</span><span style="color:gray; font-size: 100%">,</span><br>  Saurabh Gupta <span style="color:gray; font-size: 85%">(UIUC)</span>
</i></span>
</td>
<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss17/p019.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a><br> <a href="https://ansivakumar.github.io/learned-visual-navigation/"><img src="{{ site.baseurl }}/images/website_link.png" alt="Paper Website" width = "33"  height = "40"/></a><br>    </td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#019</td>
</tr>
</table>

<table width="80%" style="margin-top: 20px; margin-left: auto; margin-right: auto;">
                                          <tr><td style="text-align:center;"><a href="{{ site.baseurl }}/program/posters2/">Interactive Poster Session II</a></td> 
                                              <td style="text-align:center;"><a href="{{ site.baseurl }}/program/posters7/">Interactive Poster Session VII</a></td></tr>
<tr><td><p style="text-align: center; font-size: 10px; margin-top: 0px;" id="eventcounter1"><a>0d 00h 00m</a></p></td><td><p style="text-align: center; font-size: 10px; margin-top: 0px;" id="eventcounter2"><a>0d 00h 00m</a></p></td></tr></table>
<br>


### Abstract
This paper describes a system for visually guided autonomous navigation of under-canopy farm robots. Low-cost under-canopy robots can drive between crop rows under the plant canopy and accomplish tasks that are infeasible for over-the-canopy drones or larger agricultural equipment. However, autonomously navigating them under the canopy presents a number of challenges: unreliable GPS and LiDAR, high cost of sensing, challenging farm terrain, clutter due to leaves and weeds, and large variability in appearance over the season and across crop types. We address these challenges by building a modular system that leverages machine learning for robust and generalizable perception from monocular RGB images from low-cost cameras, and model predictive control for accurate control in challenging terrain. Our system, CropFollow, is able to autonomously drive 485 meters per intervention on average, outperforming a state-of-the-art LiDAR based system (286 meters per intervention) in extensive field testing spanning over 25 km.
{: style="color:gray; font-size: 120%; text-align: justified;"}




### Spotlight Presentation
<center><span style="font-size:smaller;"><i>This video will be released on July 12th.</i></span></center><br>
<iframe width="100%" height="400" src="https://www.youtube.com/embed/PdU_ZC6Jvck" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Links
* [Project page](https://ansivakumar.github.io/learned-visual-navigation/)


<table width="100%" style="margin-top:40px;">
 <tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/018/">
<img src="{{ site.baseurl }}/images/previous_paper_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/020/">
    <img src="{{ site.baseurl }}/images/next_paper_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
</tr>
</table>


<script>
var startDate1 = new Date("2021-07-13 07:00:00 UTC-0700").getTime();
var finDate1 = new Date("2021-07-13 08:15:00 UTC-0700").getTime();

// Update the count down every 1 second
var x1 = function() {

  // Get today's date and time
  var now1 = new Date().getTime();
    
  var distToStart1 = startDate1 - now1;
  if (distToStart1 > 0) {

      var days = Math.floor(distToStart1 / (1000 * 60 * 60 * 24));
      var hours = Math.floor((distToStart1 % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
      var minutes = Math.floor((distToStart1 % (1000 * 60 * 60)) / (1000 * 60));
   
      document.getElementById("eventcounter1").innerHTML = "<a><span style='color: #aaaaaa;'>" + days + "d " + hours + "h " + minutes + "m</span></a>" ;
      setTimeout(x1, 5000); 
    
  } else {

        var distToEnd1 = finDate1 - now1;

        if (distToEnd1 > 0) {
            document.getElementById("eventcounter1").innerHTML = '<img src="{{ site.baseurl }}/images/live-icon-small.gif" alt="Event is Live" width="64" height=17"><a><span style="color: #ffaaaa;">'+ distToEnd1 +'</span></a> ';
            setTimeout(x1, 30000); 
        }
        else
        { 
            document.getElementById("eventcounter1").innerHTML = "<a><span style='color: #aaaaaa;'>Now concluded</span></a>";
        }
  }
};

setTimeout(x1,0);
</script>

    
<script>
var startDate2 = new Date("2021-07-15 08:15:00 UTC-0700").getTime();
var finDate2 = new Date("2021-07-15 09:30:00 UTC-0700").getTime();

// Update the count down every 1 second
var x2 = function() {

  // Get today's date and time
  var now2 = new Date().getTime();
    
  var distToStart2 = startDate2 - now2;
  if (distToStart2 > 0) {

      var days = Math.floor(distToStart2 / (1000 * 60 * 60 * 24));
      var hours = Math.floor((distToStart2 % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
      var minutes = Math.floor((distToStart2 % (1000 * 60 * 60)) / (1000 * 60));
   
      document.getElementById("eventcounter2").innerHTML = "<a><span style='color: #aaaaaa;'>" + days + "d " + hours + "h " + minutes + "m</span></a>" ;
      setTimeout(x2, 5000); 
    
  } else {

        var distToEnd2 = finDate2 - now2;

        if (distToEnd2 > 0) {
            document.getElementById("eventcounter2").innerHTML = '<img src="{{ site.baseurl }}/images/live-icon-small.gif" alt="Event is Live" width="64" height=17"><a><span style="color: #ffaaaa;">'+ distToEnd2 +'</span></a> ';
            setTimeout(x2, 30000); 
        }
        else
        { 
            document.getElementById("eventcounter2").innerHTML = "<a><span style='color: #aaaaaa;'>Now concluded</span></a>";
        }
  }
};

setTimeout(x2,0);
</script>

    