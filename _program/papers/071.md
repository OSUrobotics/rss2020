---
layout: paper
title: "Ab Initio Particle-based Object Manipulation"
invisible: true
---
<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Siwei Chen <span style="color:gray; font-size: 85%">(National University of Singapore)</span><span style="color:gray; font-size: 100%">,</span><br>  Xiao Ma <span style="color:gray; font-size: 85%">(National University of Singapore)</span><span style="color:gray; font-size: 100%">,</span><br>  Yunfan Lu <span style="color:gray; font-size: 85%">(National University of Singapore)</span><span style="color:gray; font-size: 100%">,</span><br>  David Hsu <span style="color:gray; font-size: 85%">(National University of Singapore)</span>
</i></span>
</td>
<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss17/p071.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br> <a href=" https://adacomp.comp.nus.edu.sg/2021/06/26/particle-based-robot-manipulation"><img src="{{ site.baseurl }}/images/website_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br>    </td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#071</td>
</tr>
</table>


### Abstract
This paper presents Particle-based Object Manipulation (PROMPT), a new approach to robot manipulation of novel objects ab initio, without prior object models or pre-training on a large object data set. The key element of PROMPT is a particle-based object representation, in which each particle represents a point in the object, the local geometric, physical, and other features of the point, and also its relation with other particles. Like the model-based analytic approaches to manipulation, the particle representation enables the robot to reason about the object's geometry and dynamics in order to choose suitable manipulation actions. Like the data-driven approaches, the particle representation is inferred online in real-time from visual sensor input, specifically, multi-view RGB images. The particle representation thus connects visual perception with robot control. PROMPT combines the benefits of both model-based reasoning and data-driven learning. We show empirically that PROMPT successfully handles a variety of everyday objects, some of which are transparent. It handles various manipulation tasks, including grasping, pushing, etc,. Our experiments also show that PROMPT outperforms a state-of-the-art data-driven grasping method on the daily objects, even though it does not use any offline training data.
{: style="color:gray; font-size: 120%; text-align: justified;"}



<table width="100%">
 <tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/070/">
<img src="{{ site.baseurl }}/images/previous_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/072/">
    <img src="{{ site.baseurl }}/images/next_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
</tr>
</table>