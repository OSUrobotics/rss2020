---
layout: paper
title: "Learning Instance-Level N-Ary Semantic Knowledge At Scale For Robots Operating in Everyday Environments"
invisible: true
---
<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Weiyu Liu <span style="color:gray; font-size: 85%">(Georgia Tech)</span><span style="color:gray; font-size: 100%">,</span><br>  Dhruva Bansal <span style="color:gray; font-size: 85%">(Georgia Tech)</span><span style="color:gray; font-size: 100%">,</span><br>  Angel Daruna <span style="color:gray; font-size: 85%">(Georgia Tech)</span><span style="color:gray; font-size: 100%">,</span><br>  Sonia Chernova <span style="color:gray; font-size: 85%">(Georgia Tech)</span>
</i></span>
</td>
<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss17/p035.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "50"  height = "60"/></a><br>    <a href="https://github.com/wliu88/LINK"><img src="{{ site.baseurl }}/images/software_link.png" alt="Code" width = "50"  height = "60"/></a><br> </td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#035</td>
</tr>
</table>


### Abstract
Robots operating in everyday environments need to effectively perceive; model; and infer semantic properties of objects. Existing knowledge reasoning frameworks only model binary relations between an object's class label and its semantic properties; unable to collectively reason about object properties detected by different perception algorithms and grounded in diverse sensory modalities. We bridge the gap between multimodal perception and knowledge reasoning by introducing an n-ary representation that models complex; inter-related object properties. To tackle the problem of collecting n-ary semantic knowledge at scale; we propose a transformer neural network that directly generalizes knowledge from observations of object instances. The learned model can reason at different levels of abstraction; effectively predicting unknown properties of objects in different environmental contexts given different amounts of observed information. We quantitatively validate our approach against five prior methods on LINK; a unique dataset we contribute that contains 1457 situated object instances with 15 multimodal properties types and 200 total properties. Compared to the prior state of the art Markov Logic Network; our model obtains a 10% improvement in predicting unknown properties of novel object instances while reducing training and inference time by 150 times. Additionally; we apply our work to a mobile manipulation robot; demonstrating its ability to leverage n-ary reasoning to retrieve objects and actively detect object properties. 
{: style="color:gray; font-size: 120%; text-align: justified;"}



<table width="100%">
 <tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/034/">
<img src="{{ site.baseurl }}/images/previous_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/036/">
    <img src="{{ site.baseurl }}/images/next_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
</tr>
</table>