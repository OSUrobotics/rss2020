---
layout: paper
title: "Risk-sensitive Inverse Reinforcement Learning via Coherent Risk Models"
comments: true
invisible: true
---

<p class="text-left"><i>Authors: Anirudha Majumdar, Sumeet Singh, Ajay Mandlekar, Marco Pavone</i></p>

The literature on Inverse Reinforcement Learning (IRL) typically assumes that humans take actions in order to minimize the expected value of a cost function, i.e., that humans are risk neutral. Yet, in practice, humans are often far from being risk neutral. To fill this gap, the objective of this paper is to devise a framework for risk-sensitive IRL in order to explicitly account for an expert's risk sensitivity. To this end, we propose a flexible class of models based on coherent risk metrics, which allow us to capture an entire spectrum of risk preferences from risk-neutral to worst-case. We propose efficient algorithms based on Linear Programming for inferring an expert's underlying risk metric and cost function for a rich class of static and dynamic decision-making settings. The resulting approach is demonstrated on a simulated driving game with ten human participants. Our method is able to infer and mimic a wide range of qualitatively different driving styles from highly risk-averse to risk-neutral in a data-efficient manner. Moreover, comparisons of the Risk-Sensitive (RS) IRL approach with a risk-neutral model show that the RS-IRL framework more accurately captures observed participant behavior both qualitatively and quantitatively.

[<b><a href="/static/papers/62.pdf">Full Paper</a></b> \| <b><a href="/static/slides/62.mp4">Slides</a></b>]

{% include disqus.html %}