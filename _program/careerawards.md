---
layout: page
title: Early Career Awards
description: Early Career Awards, keynote information and speaker bio.
priority: 7
invisible: false
---

It is our great pleasure to announce this year's Early Career
Awards. The three awardees will give live plenary keynotes with Q&A on
July 14, 15, and 16, respectively. Additional live Q&As with the
awardees in Eastern time zones will be held on the following day.

<table class="table">

<tr>
<th style="text-align:center;">
	<img src="{{site.baseurl }}/images/career_boots.jpg" width = "250"/>
	<br>
	<a href="https://homes.cs.washington.edu/~bboots/">
		Byron Boots
	</a>
	<br>
	<i><span style="font-weight:normal">University of Washington </span></i>
	<br/>
	<b>July 14, 18:30 UTC</b>
</th>
<th>
	<b>Biography:</b>
	<p style="text-align:justify; font-weight:normal;">
	
Byron Boots is an Associate Professor in the Paul G. Allen School of
Computer Science and Engineering at the University of Washington. He
received his PhD from the Machine Learning Department in the School of
Computer Science at Carnegie Mellon University in 2012. He joined the
University of Washington as a postdoctoral researcher from 2012-2014,
and was an Assistant Professor in the School of Interactive Computing
at Georgia Tech from 2014-2019. His group performs fundamental and
applied research in machine learning, artificial intelligence, and
robotics with a focus on developing theory and systems that tightly
integrate perception, learning, and control. His work has been applied
to a range of problems including localization and mapping, motion
planning, robotic manipulation, and high-speed navigation. Byron has
received several awards including the 2010 ICML Best Paper Award, the
2018 AISTATS Best Paper Award, the 2019 RSS Best Student Paper Award,
and the IJRR Paper of the Year Award for 2018. He is also the
recipient of the NSF CAREER Award (2018), the Amazon Research Award
(2019), and the Outstanding Junior Faculty Research Award from the
College of Computing at Georgia Tech (2019).

	</p>
</th>
</tr>

<tr>
<th style="text-align:center;">
	<img src="{{site.baseurl }}/images/career_carlone.jpeg" width = "250"/>
	<br>
	<a href="https://lucacarlone.mit.edu/">
        Luca Carlone 
	</a>
	<br>
	<i><span style="font-weight:normal">Massachusetts Institute of Technology </span></i>
	<br/>
	<b>July 15, 18:30 UTC</b>
</th>
<th>
	<b>Title:</b>
	<p style="text-align:justify; font-weight:normal;">
The Future of Robot Perception: Certifiable Algorithms and Real-time High-level Understanding
</p>

	<b>Abstract:</b>
	<p style="text-align:justify; font-weight:normal;">

Robot perception has witnessed an unprecedented progress in the last decade. Robots are now able to detect objects and create large-scale maps of an unknown environment, which are crucial capabilities for navigation, manipulation, and human-robot interaction. Despite these advances, both researchers and practitioners are well aware of the brittleness of current perception systems, and a large gap still separates robot and human perception. 
<br/>
This talk discusses two efforts targeted at bridging this gap. The first focuses on robustness. I present recent advances in the design of certifiable perception algorithms that are robust to extreme amounts of noise and outliers and afford performance guarantees. I present fast certifiable algorithms for object pose estimation: our algorithms are “hard to break” (e.g., are robust to 99% outliers) and succeed in localizing objects where an average human would fail. Moreover, they come with a “contract” that guarantees their input-output performance. I discuss the foundations of certifiable perception and motivate how these foundations can lead to safer systems.
<br/>
The second effort targets high-level understanding. While humans are able to quickly grasp both geometric, semantic, and physical aspects of a scene, high-level scene understanding remains a challenge for robotics. I present our work on real-time metric-semantic understanding and 3D Dynamic Scene Graphs. I introduce the first generation of Spatial Perception Engines, that extend the traditional notions of mapping and SLAM, and allow a robot to build a “mental model” of the environment, including spatial concepts (e.g., humans, objects, rooms, buildings) and their relations at multiple levels of abstraction.
<br/>
Certifiable algorithms and real-time high-level understanding are key enablers for the next generation of autonomous systems, that are trustworthy, understand and execute high-level human instructions, and operate in large dynamic environments and over and extended period of time.

</p>

	<b>Biography:</b>
	<p style="text-align:justify; font-weight:normal;">
	
	Luca Carlone is the Charles Stark Draper Assistant Professor in
	the Department of Aeronautics and Astronautics at the
	Massachusetts Institute of Technology, and a Principal
	Investigator in the Laboratory for Information & Decision Systems
	(LIDS). He received his PhD from the Polytechnic University of
	Turin in 2012. He joined LIDS as a postdoctoral associate (2015)
	and later as a Research Scientist (2016), after spending two years
	as a postdoctoral fellow at the Georgia Institute of Technology
	(2013-2015). His research interests include nonlinear estimation,
	numerical and distributed optimization, and probabilistic
	inference, applied to sensing, perception, and decision-making in
	single and multi-robot systems. His work includes seminal results
	on certifiably correct algorithms for localization and mapping, as
	well as approaches for visual-inertial navigation and distributed
	mapping. He is a recipient of the 2017 Transactions on Robotics
	King-Sun Fu Memorial Best Paper Award, the best paper award at
	WAFR’16, the best Student paper award at the 2018 Symposium on
	VLSI Circuits, the best paper award in Robotic Vision at ICRA'20,
	and he was best paper finalist at RSS’15. He is also the recipient
	of the Google Daydream (2019) and the Amazon Research Award
	(2020), and the MIT AeroAstro Vickie Kerrebrock Faculty Award
	(2020). At MIT, he teaches “Robotics: Science and Systems,” the
	introduction to robotics for MIT undergraduates, and he created
	the graduate-level course “Visual Navigation for Autonomous
	Vehicles”, which covers mathematical foundations and fast C++
	implementations of spatial perception algorithms for drones and
	autonomous vehicles.
	
	</p>
</th>
</tr>

<tr>
<th style="text-align:center;">
	<img src="{{site.baseurl }}/images/career_bohg.png" width = "250"/>
	<br>
	<a href="https://web.stanford.edu/~bohg/">
        Jeannette Bohg
	</a>
	<br>
	<i><span style="font-weight:normal">Standford University </span></i>
	<br/>
	<b>July 16, 17:00 UTC</b>
</th>
<th>
	<b>Biography:</b>
	<p style="text-align:justify; font-weight:normal;">
	
	Jeannette Bohg is an Assistant Professor of Computer Science at Stanford
	University. She was a group leader at the Autonomous Motion Department
	(AMD) of the MPI for Intelligent Systems until September 2017. Before
	joining AMD in January 2012, Jeannette Bohg was a PhD student at the
	Division of Robotics, Perception and Learning (RPL) at KTH in Stockholm.
	In her thesis, she proposed novel methods towards multi-modal scene
	understanding for robotic grasping. She also studied at Chalmers in
	Gothenburg and at the Technical University in Dresden where she received
	her Master in Art and Technology and her Diploma in Computer Science,
	respectively. Her research focuses on perception and learning for
	autonomous robotic manipulation and grasping. She is specifically
	interesting in developing methods that are goal-directed, real-time and
	multi-modal such that they can provide meaningful feedback for execution
	and learning. Jeannette Bohg has received several awards, most notably
	the 2019 IEEE International Conference on Robotics and Automation (ICRA)
	Best Paper Award, the 2019 IEEE Robotics and Automation Society Early
	Career Award and the 2017 IEEE Robotics and Automation Letters (RA-L)
	Best Paper Award.
	
	</p>
</th>
</tr>

</table>




